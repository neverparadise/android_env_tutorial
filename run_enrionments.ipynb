{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "import android_env\n",
    "from dm_env import specs\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print(os.curdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 load 함수를 통해 안드로이드 환경을 불러옵니다. \n",
    "# 에뮬레이터가 실행됩니다.\n",
    "original_env = android_env.load(\n",
    "      emulator_path='~/Android/Sdk/emulator/emulator',\n",
    "      android_sdk_root='~/Android/Sdk',\n",
    "      android_avd_home='~/.android/avd',\n",
    "      avd_name='my_avd',\n",
    "      adb_path='~/Android/Sdk/platform-tools/adb',\n",
    "      task_path=f'{os.curdir}/tasks/mdp/mdp_0000.textproto',\n",
    "      #task_path=f'{os.curdir}/tasks/mdp/mdp_0003.textproto',\n",
    "      run_headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from android_env.wrappers.discrete_action_wrapper import DiscreteActionWrapper\n",
    "from android_env.wrappers.image_rescale_wrapper import ImageRescaleWrapper\n",
    "from android_env.wrappers.float_pixels_wrapper import FloatPixelsWrapper\n",
    "from android_env.wrappers.tap_action_wrapper import TapActionWrapper\n",
    "\n",
    "def make_env(env):\n",
    "    print('-'*128)\n",
    "    print(env.action_spec())\n",
    "    print()\n",
    "    print(env.observation_spec())  \n",
    "    print()\n",
    "    \n",
    "    env = ImageRescaleWrapper(env, zoom_factors=(0.0625, 0.0745),  grayscale=True)\n",
    "    print('-'*128)\n",
    "    print(env.action_spec())\n",
    "    print()\n",
    "    print(env.observation_spec())  \n",
    "    print()\n",
    "    \n",
    "    env = TapActionWrapper(env, touch_only=True)\n",
    "    print('-'*128)\n",
    "    print(env.action_spec())\n",
    "    print()\n",
    "    print(env.observation_spec())  \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    env = DiscreteActionWrapper(env, (6, 9), redundant_actions=False) # action touch grid: 54 blocks\n",
    "    print('-'*128)\n",
    "    print(env.action_spec())\n",
    "    print()\n",
    "    print(env.observation_spec())  \n",
    "    print()\n",
    "    \n",
    "    env = FloatPixelsWrapper(env)\n",
    "    \n",
    "    \n",
    "\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "{'action_type': DiscreteArray(shape=(), dtype=int32, name=action_type, minimum=0, maximum=2, num_values=3), 'touch_position': BoundedArray(shape=(2,), dtype=dtype('float32'), name='touch_position', minimum=[0. 0.], maximum=[1. 1.])}\n",
      "\n",
      "{'pixels': Array(shape=(1920, 1080, 3), dtype=dtype('uint8'), name='pixels'), 'timedelta': Array(shape=(), dtype=dtype('int64'), name='timedelta'), 'orientation': Array(shape=(4,), dtype=dtype('uint8'), name='orientation')}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "{'action_type': DiscreteArray(shape=(), dtype=int32, name=action_type, minimum=0, maximum=2, num_values=3), 'touch_position': BoundedArray(shape=(2,), dtype=dtype('float32'), name='touch_position', minimum=[0. 0.], maximum=[1. 1.])}\n",
      "\n",
      "{'pixels': Array(shape=(120, 80, 1), dtype=dtype('uint8'), name='pixels'), 'timedelta': Array(shape=(), dtype=dtype('int64'), name='timedelta'), 'orientation': Array(shape=(4,), dtype=dtype('uint8'), name='orientation')}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "{'action_type': DiscreteArray(shape=(), dtype=int32, name=action_type, minimum=0, maximum=0, num_values=1), 'touch_position': BoundedArray(shape=(2,), dtype=dtype('float32'), name='touch_position', minimum=[0. 0.], maximum=[1. 1.])}\n",
      "\n",
      "{'pixels': Array(shape=(120, 80, 1), dtype=dtype('uint8'), name='pixels'), 'timedelta': Array(shape=(), dtype=dtype('int64'), name='timedelta'), 'orientation': Array(shape=(4,), dtype=dtype('uint8'), name='orientation')}\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "{'action_id': DiscreteArray(shape=(), dtype=int32, name=action_id, minimum=0, maximum=53, num_values=54)}\n",
      "\n",
      "{'pixels': Array(shape=(120, 80, 1), dtype=dtype('uint8'), name='pixels'), 'timedelta': Array(shape=(), dtype=dtype('int64'), name='timedelta'), 'orientation': Array(shape=(4,), dtype=dtype('uint8'), name='orientation')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# task의 observation과 action에 대한 정보를 보겠습니다.\n",
    "env = make_env(original_env)\n",
    "action_spec = env.action_spec() \n",
    "obs_spec = env.observation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dm_env.specs.BoundedArray'>\n",
      "0.0\n",
      "1.0\n",
      "(120, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(obs_spec['pixels']))\n",
    "print(obs_spec['pixels'].minimum)\n",
    "print(obs_spec['pixels'].maximum)\n",
    "print(obs_spec['pixels'].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 환경을 초기화합니다. 에피소드 처음부터 시작하게 됩니다.\n",
    "\n",
    "# import dm_env\n",
    "# import random\n",
    "# DONE = dm_env.StepType.LAST\n",
    "# #action['touch_position'] = action_index\n",
    "# time_step = env.reset()\n",
    "# print(_.step_type)\n",
    "\n",
    "# while time_step.step_type != DONE:\n",
    "#     action = {}\n",
    "#     action['action_id'] = random.randint(0, 54)\n",
    "#     time_step = env.step(action)\n",
    "#     if time_step.step_type == DONE:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step_type, reward, time_delta, obs = time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(step_type)\n",
    "# print(reward)\n",
    "# print(time_delta)   \n",
    "# # print(obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(_))\n",
    "# print(_[0])\n",
    "# print(_[1])\n",
    "# print(_[2])\n",
    "\n",
    "# print(_.reward)\n",
    "# print(_.discount)\n",
    "# print(_.observation['pixels'].shape)\n",
    "# print(_.observation['pixels'][0])\n",
    "# print(_.observation['pixels'][0].shape)\n",
    "# print(_.observation['orientation'])\n",
    "# print(_.observation['timedelta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_action() -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Returns a random AndroidEnv action.\"\"\"\n",
    "    action = {}\n",
    "    for k, v in action_spec.items():\n",
    "        if isinstance(v, specs.DiscreteArray):\n",
    "            action[k] = np.random.randint(low=0, high=v.num_values, dtype=v.dtype)\n",
    "        else:\n",
    "            action[k] = np.random.random(size=v.shape).astype(v.dtype)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from models.DQN import ClassicCNN, train_dqn\n",
    "from buffer.replay_buffer import ReplayBuffer\n",
    "from utils import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_id': DiscreteArray(shape=(), dtype=int32, name=action_id, minimum=0, maximum=53, num_values=54)}\n",
      "54\n",
      "{'pixels': Array(shape=(120, 80, 1), dtype=dtype('uint8'), name='pixels'), 'timedelta': Array(shape=(), dtype=dtype('int64'), name='timedelta'), 'orientation': Array(shape=(4,), dtype=dtype('uint8'), name='orientation')}\n",
      "Array(shape=(120, 80, 1), dtype=dtype('uint8'), name='pixels')\n",
      "(120, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_spec())\n",
    "print(env.action_spec()['action_id'].num_values)\n",
    "print(env.observation_spec())\n",
    "print(env.observation_spec()['pixels'])\n",
    "print(env.observation_spec()['pixels'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('action_id', DiscreteArray(shape=(), dtype=int32, name=action_id, minimum=0, maximum=53, num_values=54))])\n"
     ]
    }
   ],
   "source": [
    "print(env.action_spec().items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dm_env\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "FIRST = dm_env.StepType.FIRST\n",
    "DONE = dm_env.StepType.LAST\n",
    "GAMMA=0.9\n",
    "MEMORY_SIZE = 50000\n",
    "BATCH_SIZE = 128   # 32\n",
    "LEARNING_RATE = 0.000625   # 0.01\n",
    "TARGET_UPDATE = 10  # 5\n",
    "SAVE_PATH = \"/home/slowlab/android_env_tutorial/weights/dqn/\"\n",
    "MODEL_NAME = 'DQN'\n",
    "ENV_NAME = 'mdp_0000'\n",
    "SAVE_PERIOD = 100\n",
    "START_SIZE = 2000\n",
    "\n",
    "\n",
    "\n",
    "def converter(obs):\n",
    "    if len(obs['pixels'].shape) < 4:\n",
    "        obs_pixels = obs['pixels']\n",
    "        obs_pixels = obs_pixels.transpose(2, 0, 1)\n",
    "        obs_tensor = torch.tensor(obs_pixels).to(device).float()\n",
    "        obs_tensor = obs_tensor.unsqueeze(0)\n",
    "    else:\n",
    "        obs_pixels = obs_pixels.transpose(2, 0, 1)\n",
    "        obs_tensor = torch.tensor(obs_pixels).to(device).float()\n",
    "        obs_tensor = obs_tensor.unsqueeze(0)\n",
    "    return obs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 80 1\n",
      "8065\n",
      "8065\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'time_delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1690172/1590795966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1690172/1590795966.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0maction_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbehavior_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/android_env_tutorial/models/DQN.py\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs_dict, eps, training)\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0mcoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcoin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time_delta'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    summary_path = \"/home/slowlab/android_env_tutorial/experiments/dqn/train{}\".format(ENV_NAME)\n",
    "    if not os.path.isdir(summary_path):\n",
    "        os.mkdir(summary_path)\n",
    "    writer = SummaryWriter(summary_path)\n",
    "    \n",
    "    startEpsilon = 0.95\n",
    "    endEpsilon = 0.05\n",
    "    total_episodes = 10000\n",
    "    #total_steps = 100000\n",
    "    epsilon = startEpsilon\n",
    "    stepDrop = (startEpsilon - endEpsilon)  * 5 / total_episodes\n",
    "    n_actions = env.action_spec()['action_id'].num_values\n",
    "    state_dim = env.observation_spec()['pixels'].shape\n",
    "    H, W, C = state_dim[0], state_dim[1], state_dim[2]\n",
    "    print(H, W, C)\n",
    "    behavior_policy = ClassicCNN(C, H, W, 3, 2, n_actions).to(device).float()    # C, H, W, K, S, num_actions\n",
    "    target_policy = ClassicCNN(C, H, W, 3, 2, n_actions).to(device).float()\n",
    "    target_policy.load_state_dict(behavior_policy.state_dict())\n",
    "    optimizer = optim.Adam(behavior_policy.parameters(), lr=LEARNING_RATE)\n",
    "    memory = ReplayBuffer(MEMORY_SIZE)\n",
    "\n",
    "    # return Timestep object (step_type, reward, time_delta, obs)\n",
    "    \n",
    "    for episode in range(total_episodes):\n",
    "        if(epsilon > endEpsilon):\n",
    "            epsilon -= stepDrop\n",
    "        total_rewards = 0\n",
    "        timestep = env.reset()\n",
    "        loss = 0\n",
    "        #step_type, reward, discount, obs = timestep\n",
    "        \n",
    "        while not timestep.last():\n",
    "            action_index = behavior_policy.sample_action(timestep.observation, epsilon)\n",
    "            action = {}\n",
    "            action['action_id'] = action_index\n",
    "            #action['touch_position'] = action_index\n",
    "        \n",
    "            next_timestep = env.step(action=action)\n",
    "            total_rewards += next_timestep.reward\n",
    "            \n",
    "            if next_timestep.step_type == DONE:\n",
    "                print(f'total_rewards of episode {episode}: {total_rewards}')\n",
    "                print(f\"# of transitions in memory: {memory.size()}\")\n",
    "                writer.add_scalar(\"total_rewards\", total_rewards, episode)\n",
    "                writer.add_scalar(\"epsilon\", epsilon, episode)\n",
    "                writer.add_scalar(\"loss\", loss, episode)\n",
    "                break\n",
    "        \n",
    "            transition = (timestep, action, next_timestep)\n",
    "            memory.put(transition)\n",
    "            timestep = next_timestep\n",
    "            if memory.size() > START_SIZE:\n",
    "                loss = train_dqn(behavior_policy, target_policy, memory, optimizer, GAMMA, BATCH_SIZE)\n",
    "    \n",
    "        if episode % TARGET_UPDATE == 0:\n",
    "            target_policy.load_state_dict(behavior_policy.state_dict())\n",
    "        save_model(episode, SAVE_PERIOD, SAVE_PATH,target_policy, MODEL_NAME)\n",
    "        \n",
    "\n",
    "    writer.close()\n",
    "main()\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model(step, SAVE_PERIOD, SAVE_PATH,target_policy, MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 80 1\n",
      "8064\n"
     ]
    }
   ],
   "source": [
    "# def eval():\n",
    "#     writer = SummaryWriter(\"/home/slowlab/android_env_tutorial/experiments/dqn/test\")\n",
    "\n",
    "#     total_steps = 10000\n",
    "#     n_actions = env.action_spec()['action_id'].num_values\n",
    "#     state_dim = env.observation_spec()['pixels'].shape\n",
    "#     H, W, C = state_dim[0], state_dim[1], state_dim[2]\n",
    "#     print(H, W, C)\n",
    "#     behavior_policy = ClassicCNN(C, H, W, 3, 2, n_actions).to(device).float()    # C, H, W, K, S, num_actions\n",
    "#     SAVE_PATH = \"/home/slowlab/android_env_tutorial/weights/dqn/DQN495000.pt\"\n",
    "#     load_model(behavior_policy, SAVE_PATH)\n",
    "    \n",
    "#     total_rewards = 0\n",
    "#     epsilon = 0.0\n",
    "#     step_type, reward, discount, obs = env.reset() # return Timestep object (step_type, reward, time_delta, obs)\n",
    "#     for step in range(total_steps):\n",
    "#         action_index = behavior_policy.sample_action(converter(obs), epsilon)\n",
    "#         action = {}\n",
    "#         action['action_id'] = action_index\n",
    "#         step_type, reward, discount, next_obs = env.step(action=action)\n",
    "#         writer.add_scalar(\"reward\", reward, step)\n",
    "#         total_rewards += reward\n",
    "#         writer.add_scalar(\"total_rewards\", total_rewards, step)\n",
    "#         obs = next_obs\n",
    "\n",
    "#     writer.close()\n",
    "# eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11404e55295a2e74b3a491fc84e8bde84fbe55455e7e792ee06c33a30005b841"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('rlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
